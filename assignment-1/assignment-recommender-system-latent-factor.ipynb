{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics in Applied Optimization (IIIT, Hyderabad, India)\n",
    "\n",
    "# Jupyter Notebook Assignment-1\n",
    "\n",
    "### Instructor: Dr. Pawan Kumar (IIIT, H) (https://faculty.iiit.ac.in/~pawan.kumar/)\n",
    "\n",
    "If you are not familiar with Jupyter notebook, before proceeding further, please go and watch this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=HW29067qVWk\n",
    "\n",
    "## Regarding assignments\n",
    "\n",
    "### Deadline: 13 October 2023\n",
    "\n",
    "- all the assignment is to be done in this notbook itself\n",
    "- any proof etc can be done on paper, and image is to be inserted in this notebook\n",
    "- save this notebook with your roll number and upload it in moodle\n",
    "- basic familiarity with python is required, brush up if necessary\n",
    "- you are not allowed to use any existing library for gradient methods; this defeats the purpose of this assignment\n",
    "- sample output is in the zip file of assignment\n",
    "- if there are any doubt, then raise it in course moodle site, it may help others\n",
    "- if you find typo, raise this issue in moodle promptly!\n",
    "- please avoid copying from others, there may be oral exam to test your knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Recommender System Using Latent Factor Model in Python\n",
    "\n",
    "Please refer to the class slides for more detail. Also, for full detail, please refer the following main reference:\n",
    "\n",
    "**[1] Y. Koren, R. Bell, and C. Volinsky, Matrix Factorization Techniques for Recommender Systems, Computer Archive, Volume 42, Issue 8, 2009**\n",
    "\n",
    "We first load the necessary libraries:\n",
    "\n",
    "1. matplotlib: needed for plotting figures and\n",
    "2. numpy needed for doing math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we write the main functions\n",
    "\n",
    "We write everything inside a class LF, which stands for latent factor. We need the following functions:\n",
    "\n",
    "1. main\n",
    "2. train\n",
    "3. mse\n",
    "4. sgd\n",
    "5. predict\n",
    "6. full_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Tutorial on Recommender Systems\n",
    "\n",
    "Please see lecture slides for more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Factor Model\n",
    "\n",
    "In a recommender systems, we are given an incomplete ratings matrix say R. The $(i,j)$ entry of this matrix is denoted by $r_{ij}.$\n",
    "\n",
    "- We believe that there are dependencies in the matrix, i.e., the matrix is low rank, hence it can be written as a product of low rank matrices.\n",
    "- Even if the matrix is not strictly low rank, we believe that certain features are more important than others. In other words, if the ratings matrix were full, then after doing SVD, we get\n",
    "  $$R = U S V^T$$\n",
    "  some of the singular values were very small, hence the matrix can be approximated well with truncated SVD. Let us consider the first $k$ largest singular vectors, then the truncated SVD is\n",
    "  $$R \\approx U_k S_k V_k.$$\n",
    "- See IPSC notes for more detail on how to compute SVD or truncated SVD\n",
    "- For a recomendation problem, for example, movie recomendation or news recommendation or product recommendation, etc, we believe that certain latent features are most important and play a major role in ratings.\n",
    "- For movie recommendation, for example, latent factors could be comedy, adventure, horror, etc. These features are called latent, because the given data, which is ratings matrix does not explicitely tells about these.\n",
    "- For recommendation systems, we cannot do SVD, because the matrix $R$ is incomplete! Obviously, we cant treat the missing entries to be zero! Hence, instead of doing SVD, we will consider an error function that consider only the ratings that are given. Let us assume that the ratings matrix can be modelled as a latent factors, i.e., suppose that $$R = PQ, \\quad P \\in \\mathbb{R}^{n \\times k}, Q \\in \\mathbb{R}^{k \\times n},$$ then the given rating $$r_{ij} = p_{i*}q_{*j},$$ where $p_{i*}$ denotes the $i$th row of $P,$ and $q_{*j}$ denotes the $j$th column of $Q.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model\n",
    "\n",
    "- As mentioned above, we cannot directly do SVD, however, we can indeed create a **loss function** as follows:\n",
    "  $$\\mathcal{L}(p_{1*}, p_{2*}, \\dots p_{n*}, q_{*1}, q_{*2}, \\dots, q_{*n}) = \\sum_{(i,j) \\in K} (r_{ij} - p_{i*}q_{*j}).$$\n",
    "- Note above that we see loss function as a function the variables $p_{1*}, p_{2*}, \\dots, p_{*n},$ which are row vectors of $P,$ and as the column vectors of $Q,$ which are $q_{*1}, q_{*2}, \\dots, q_{*n}.$ This allows us to **vectorize**. Why vectorize?\n",
    "- In machine learning the weights and their combinations with the given data creates a model. In this case, the model is $PQ.$ Since the $k,$ is a hyperparameter, it is likely that one may choose $P$ and $Q$ to have too many columns, i.e., too many weights are used, this will lead to a large model, and hence leading to **overfitting**.\n",
    "  If you are not familiar with overfitting, then pause here, and see the video here:\n",
    "\n",
    "https://www.youtube.com/watch?v=u73PU6Qwl1I\n",
    "\n",
    "    - What did you learn from video?\n",
    "    - What is termed as model?\n",
    "    - What is the meaning of model being large?\n",
    "    - What is the difference between linear and logistic regression?\n",
    "    - What is underfitting?\n",
    "    - What is a bias?\n",
    "    - How do you characterize bias and variance using terms overfitting or underfitting?\n",
    "\n",
    "- A well known way to avoid overfitting is to do regularizations, by penalizing **large model** size. Let us modify our loss function as follows:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}(\\cdot) = \\sum_{(i,j) \\in K} (r_{ij} - p_{i*}q_{*j})^2 + \\gamma/2 (\\| P \\|_F^2 + \\| Q \\|_F^2)\n",
    "\n",
    "  $$ - We have added the term $\\gamma/2(\\| P \\|_F^2 + \\| Q \\|_F^2).$ It is called **regularization term** - he parameter $\\gamma$ is called the **regularization parameter** - High $\\gamma$ would mean that we don't want big model size; this may help prevent overfitting\n",
    "  $$\n",
    "\n",
    "- Let $e_{ij}$ denote the error corresponding to each $(ij)$ term,\n",
    "  $$e_{ij} = r_{ij} - \\sum_{s=1}^k p_{is}q_{sj}$$\n",
    "  then let us define the new error term $\\tilde{e}$\n",
    "  $$\\tilde{e}_{ij} = e_{ij}^2 + \\gamma/2 (\\|P\\|_F^2 + \\| Q \\|_F^2),$$\n",
    "  that is,\n",
    "  $$\\tilde{e}_{ij} = (r_{ij} - \\sum_{s=1}^k p_{is}q_{sj} )^2 + \\gamma/2 (\\|P\\|_F^2 + \\| Q \\|_F^2)$$\n",
    "- Note that $$\\mathcal{L}(\\cdot) = \\sum_{(i,j) \\in \\mathcal{K}} \\tilde{e}_{ij},$$\n",
    "  where $\\mathcal{K}$ is the set of all indices for which ratings $r_{ij}$ are available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Method Versus Stochastic Gradient Methods\n",
    "\n",
    "Following notes are taken from\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "For large dataset, $k$ and $n$ tend to be large, and computing gradient fully becomes too demanding. In machine learning or statistical estimation, we are usually required to find minimum of the loss functions of the form\n",
    "$$Q(w) = 1/n \\sum_{j=1}^n Q_j (w)$$\n",
    "That is loss function as \"additive\" decomposition. In other words, the loss function is a sum of the loss function where the sum is over the data samples. If $n$ is large, then computing full gradient is computationally demanding. However, if we indeed decide to run the gradient method. Then we recall the following steps:\n",
    "\n",
    "1. Take initial random weight (possibly random and normally distributed between 0 and 1): $w^0$\n",
    "2. $w^{i+1} = w^i - \\alpha \\nabla Q(w^i), \\quad i=0, \\dots$\n",
    "\n",
    "Here the gradient $\\nabla Q$ is the full gradient in the sense that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla Q(w) = \\sum{i=1}^n \\nabla{w}Q_i(w). \\label{grad} \\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here\n",
    "\\begin{align}\n",
    "\\nabla_w Q_i (w) = \\begin{bmatrix}\n",
    "\\dfrac{\\partial Q_i (w)}{\\partial w_1} \\\\\n",
    "\\dfrac{\\partial Q_i (w)}{\\partial w_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial Q_i (w)}{\\partial w_n} \\\\\n",
    "\\end{bmatrix} \\label{grad2} \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "### Stochastic gradient descent method\n",
    "\n",
    "Stochastic gradient merthod, in short, SGD is a method (not necessarily descent due to stochasticity, but in expectation is usually is a descent), where the gradient is computed partially. For example, instead of summing for all $i=1...n,$ we can take a random sample, say $j_1,$ and do the update only for this sample as follows:\n",
    "$$\\nabla Q(w) = \\nabla_{w}Q_{j_1}(w),$$\n",
    "that is we have picked just one of the term i.e., $j_1$th term from \\eqref{grad}.\n",
    "Then the SGD update is:\n",
    "$$w^{i+1} = w^i - \\alpha \\nabla Q_{j_1}(w^i), \\quad i=0, \\dots$$\n",
    "Note that we could have taken few more terms in the sum, and would have updated for a batch. This is called, batched stochastic gradient descent.\n",
    "It sounds incorrect, isnt it? Why such method may converge? With some basic assumption on the loss function, we can prove some convergence in expectance. More on convergence theory in class!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-0:}}$ Compute the full gradient of the loss function for recommendation system\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$\n",
    "\n",
    "Given the loss function for the recommendation system:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p, q) = e_{ij}^2 + \\gamma (||p||^2 + ||q||^2)\n",
    "$$\n",
    "\n",
    "where the error term $ e\\_{ij} $ is given by:\n",
    "\n",
    "$$\n",
    "e_{ij} = r_{ij} - p^T q\n",
    "$$\n",
    "\n",
    "The gradient with respect to the latent factors $ p $ and $ q $ are:\n",
    "\n",
    "$$\n",
    "\\nabla_p \\mathcal{L} = -2 e_{ij} q + 2\\gamma p\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_q \\mathcal{L} = -2 e_{ij} p + 2\\gamma q\n",
    "$$\n",
    "\n",
    "Here, $ \\nabla_p \\mathcal{L} $ represents the gradient of the loss function with respect to $ p $ and $ \\nabla_q \\mathcal{L} $ represents the gradient with respect to $ q $. These gradients guide the updates to the latent factors during the optimization process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we derive a stochastic gradient for the loss function for recommender loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Method for Optimization Problem\n",
    "\n",
    "- We want to use gradient method to minimize the error. An update in gradient method is given as follows\n",
    "  $$w^{i+1} = w^i - \\nabla \\mathcal{L}(w^i)$$\n",
    "- To use gradient method, we need to compute gradient. If we see $\\mathcal{L}$ as a function of $p_{ik}'$s and $q_{ki}'$s, then\n",
    "  $$\\dfrac{\\partial \\mathcal{L}}{\\partial p_{is}} = 2e_{is} \\dfrac{\\partial e_{is}}{\\partial p_{is}} + \\gamma p_{is} = -2e_{is} q_{sj} + \\gamma p_{is}$$\n",
    "  Similarly,\n",
    "  $$\\dfrac{\\partial \\mathcal{L}}{\\partial q_{sj}} = 2e_{sj} \\dfrac{\\partial e_{sj}}{\\partial q_{sj}}+\\gamma q_{sj} = -2e_{sj}p_{js} + \\gamma q_{sj},$$\n",
    "- The gradient vector $\\nabla \\mathcal{L}$ is given by\n",
    "  $$\\nabla \\mathcal{L} = \\left(\\dfrac{\\partial \\mathcal{L}}{\\partial p_{11}}, \\dots, \\dfrac{\\partial \\mathcal{L}}{\\partial p_{nk}}, \\dfrac{\\partial \\mathcal{L}}{\\partial q_{11}}, \\dots, \\dfrac{\\partial \\mathcal{L}}{\\partial p_{kn}} \\right)$$\n",
    "- The weights $p_{is}$ can be updated as\n",
    "  $$p_{is} = p_{is} + \\alpha (2e_{ij} q_{sj} - \\gamma p_{is}) $$\n",
    "- The weights $p_{sj}$ can be updated as\n",
    "  $$q_{sj} = q_{sj} + \\alpha (2e_{ij} p_{js} - \\gamma q_{sj}) $$\n",
    "- We can vectorize $s.$ Finally, the vectorized form of the update looks like:\n",
    "  $$(p_{i+1*}, q_{*j+1})^T = (p_{i*}, q_{*j})^T + \\alpha ((2e_{ij} q_{*j} - \\gamma p_{i*}), (2e_{ij} p_{j*} - \\gamma q_{*j}))^T  $$\n",
    "- The error at $(p_{i*}, q_{*j})^T$ nedded above can be computed by $$e_{ij} =  r_{ij} - \\text{prediction at}~ (i,j) $$\n",
    "  Here prediction at $(i,j)$ can be computed by making a call to predict() function below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-1:}}$ Why the error is computed this way?\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Either use latex (preferred) or write it on paper, take pic, and insert it here. See the video on jupyter above on how to paste images.\n",
    "\n",
    "The error $e_{ij}$ is computed as the difference between the actual rating $ r\\_{ij} $ and the predicted rating for user i and item j. In mathematical terms:\n",
    "\n",
    "$$\n",
    "\n",
    "e_{ij} = r_{ij} - \\text{{prediction at}}~ (i,j)\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "The rationale for this computation is:\n",
    "\n",
    "1. **Objective of Matrix Factorization:** The primary goal of matrix factorization in recommender systems is to approximate the original ratings matrix as closely as possible. By decomposing this matrix into two lower-dimensional matrices (which represent the latent factors for users and items), the predicted rating is obtained as the dot product of the user and item latent vectors.\n",
    "\n",
    "2. **Quantifying Deviation:** To gauge the accuracy of our matrix factorization, we need a measure to quantify the deviation between our predictions and the actual ratings. This deviation (or error) for a user-item pair is the difference between its actual rating and the predicted one.\n",
    "\n",
    "3. **Optimization:** During the optimization phase (like gradient descent), the goal is to minimize the cumulative error across all user-item pairs. Defining the error as the difference between actual and predicted ratings ensures the optimization process tweaks the latent factors to make the predicted ratings converge to the actual ones.\n",
    "\n",
    "In essence, this method of computing error offers a direct measure of prediction accuracy, crucial for optimizing latent factors in matrix factorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-2:}}$ Are the updates for $p_{i*}$ and $q_{*j}$ stochastic? Justify.\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Write your answer here.\n",
    "Yes, the updates for $p_{i*}$ and $q_{*j}$ in the `sgd` function are stochastic. The justification is as follows:\n",
    "\n",
    "1. **Definition of Stochastic Gradient Descent (SGD):** In SGD, rather than using the entire dataset to compute the gradient and update the weights (as in batch gradient descent), we update the weights based on the gradient computed from a single data point (or a small batch of data points).\n",
    "\n",
    "2. **Updates in the `sgd` method:** In the provided code for the `sgd` method, we iterate over the samples (i.e., the known ratings in the ratings matrix). For each sample, we:\n",
    "\n",
    "   - Compute the prediction and the error for that specific user-item pair.\n",
    "   - Update the user and item latent factors based on this error.\n",
    "\n",
    "   This means we're using a single data point (rating) at a time to compute the gradient and update the latent factors, which is characteristic of SGD.\n",
    "\n",
    "3. **Advantages of Stochastic Updates:** Using stochastic updates can help escape local minima and can lead to faster convergence, especially when the dataset is large. However, the path to convergence can be noisier compared to batch gradient descent.\n",
    "\n",
    "In conclusion, the updates for $p_{i*}$ and $q_{*j}$ in the `sgd` method are stochastic because they are based on individual samples rather than the entire dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Bias\n",
    "\n",
    "It is often observed in practice that adding a bias term helps. Let us add the bias term $b$ to the prediction function. The bias term is computed as the mean of the ratings matrix. See the predict function below to see how bias is added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-2b:}}$ Read the main reference paper and justify why bias is added. Note in this notebook we add global bias. Which other bias term was suggested in the paper?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Answer:}}$\n",
    "\n",
    "1. **Global Bias $ \\mu $**:\n",
    "   This is the overall average rating across all items and users. It represents a general baseline rating that can be expected without knowing anything about a specific user or item.\n",
    "\n",
    "2. **User Bias $ b_u $**:\n",
    "   This represents the deviation of a particular user from the average. For instance, some users might generally rate movies more critically than others. This bias term captures such tendencies.\n",
    "\n",
    "3. **Item Bias $ b_i $**:\n",
    "   Similarly, this captures the deviation of a particular item from the average. Some items might be universally acclaimed and tend to receive higher ratings, while others might be polarizing or generally not as well-received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LF():\n",
    "    def __init__(self, R, K, alpha, gamma, iterations, bias=True):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "        self.bias = bias\n",
    "\n",
    "    def train(self, tp=0):\n",
    "        self.P = np.random.normal(\n",
    "            scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(\n",
    "            scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)]) if self.bias else 0\n",
    "\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            if tp == 0:\n",
    "                self.sgd()\n",
    "            else:\n",
    "                self.gd_All()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 10 == 0:\n",
    "                if tp == 0:\n",
    "                    print(\"SGD Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "                else:\n",
    "                    print(\"GD Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.predict(i, j)\n",
    "            e = (r - prediction)\n",
    "            # ic(self.P[i, :].shape)\n",
    "            # ic(self.Q[j, :].shape)\n",
    "            # ic(e.shape)\n",
    "\n",
    "            # ic(self.b.shape)\n",
    "            self.P[i, :] += self.alpha * \\\n",
    "                (e * self.Q[j, :] - self.gamma * self.P[i, :])\n",
    "            self.Q[j, :] += self.alpha * \\\n",
    "                (e * self.P[i, :] - self.gamma * self.Q[j, :])\n",
    "\n",
    "    def gd_All(self):\n",
    "        # Initialize gradients for P and Q\n",
    "        grad_P = np.zeros((self.num_users, self.K))\n",
    "        grad_Q = np.zeros((self.num_items, self.K))\n",
    "\n",
    "        predictions = self.full_matrix()\n",
    "        observed_ratings_mask = (self.R > 0)\n",
    "\n",
    "        errors = (self.R - predictions) * observed_ratings_mask\n",
    "\n",
    "        # Compute gradients for P and Q using vectorized operations\n",
    "        for j in range(self.num_items):\n",
    "            grad_P += -2 * \\\n",
    "                np.outer(errors[:, j], self.Q[j, :]) + 2 * self.gamma * self.P\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            grad_Q += -2 * \\\n",
    "                np.outer(errors[i, :], self.P[i, :]) + 2 * self.gamma * self.Q\n",
    "\n",
    "        # Update user and item latent feature matrices using the computed gradients\n",
    "        self.P -= self.alpha * grad_P\n",
    "        self.Q -= self.alpha * grad_Q\n",
    "\n",
    "    def predict(self, i, j):\n",
    "        return self.b + self.P[i, :].dot(self.Q[j, :].T)\n",
    "\n",
    "    def full_matrix(self):\n",
    "        return self.b + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "\n",
    "lf = LF(R, K=2, alpha=0.1, gamma=0.01, iterations=100)\n",
    "training_process = lf.train()\n",
    "print()\n",
    "print(\"P x Q:\")\n",
    "print(lf.full_matrix())\n",
    "print()\n",
    "print(\"Global bias:\")\n",
    "print(lf.b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-3:}}$ Implement the full gradient descent method\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Complete gd function above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-4:}}$ Vectorize the updates of p and q, by vectorizing s (detailed above)\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Fill the TODO in code above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the code below, answer the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x, y in training_process]\n",
    "y = [y for x, y in training_process]\n",
    "plt.figure(figsize=((16, 4)))\n",
    "plt.plot(x, y)\n",
    "plt.xticks(x, x)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha changed to 0.03 instead of 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-5:}}$ Plot the MSE versus iterations for $\\alpha=1, \\gamma=0.01$ and 100 iterations of SGD\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Put your figure here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_and_plot(R, K=2, alpha=0.1, gamma=0.01, iterations=100, bias=True, tp=0):\n",
    "    lf = LF(R, K=K, alpha=alpha, gamma=gamma, iterations=iterations, bias=bias)\n",
    "    training_process = lf.train(tp=tp)\n",
    "\n",
    "    print(\"\\nP x Q:\")\n",
    "    print(lf.full_matrix())\n",
    "    print(\"\\nGlobal bias:\")\n",
    "    print(lf.b)\n",
    "    print()\n",
    "\n",
    "    x = [x for x, y in training_process]\n",
    "    y = [y for x, y in training_process]\n",
    "    plt.figure(figsize=((16, 4)))\n",
    "    plt.plot(x, y)\n",
    "    plt.xticks(x, x)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Square Error\")\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_and_plot(R, K=2, alpha=0.03, gamma=0.01, iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-6:}}$ Show the plot for 50 iterations, $\\alpha=1, \\gamma=0.01$ of SGD without bias, i.e., $b=0$\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Paste your output and figures here or below this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(R, K=2, alpha=0.03, gamma=0.01, iterations=50, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-7:}}$ Repeat above with bias\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Paste your output and figures here or below this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(R, K=2, alpha=0.03, gamma=0.01, iterations=50, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-8:}}$ Show plots with values of regularization parameters to be 1, 0.1, 0.01, 0.001 and for 50 iterations, and $\\alpha=1, \\gamma=0.01$\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Paste all your plots here or below this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.001, 0.01, 0.1, 1]:\n",
    "    print(f\"Gamma: {gamma}\")\n",
    "    train_and_plot(R, K=2, alpha=0.03, gamma=gamma, iterations=50, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-9:}}$ Prove that the loss function is not convex. Is the loss function differentiable?\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Write your answer here.\n",
    "\n",
    "Before delving into the analysis, it's worth noting that we are considering a special case of the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p, q) = -pq + \\gamma (p^2 + q^2)\n",
    "$$\n",
    "\n",
    "This examination is significant because if the function is not convex for this special case when R has zeroes everywhere, it implies that the function is not convex for the general case as well.\n",
    "\n",
    "## Convexity:\n",
    "\n",
    "### Hessian Matrix:\n",
    "\n",
    "The Hessian matrix for the function is computed as:\n",
    "\n",
    "$$\n",
    "H(\\mathcal{L}) = \\begin{bmatrix}\n",
    "2\\gamma & -1 \\\\\n",
    "-1 & 2\\gamma \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Eigenvalues:\n",
    "\n",
    "By deriving the characteristic polynomial of the Hessian, we deduced the eigenvalues:\n",
    "\n",
    "$$\n",
    "\\lambda_1 = \\frac{4\\gamma + \\sqrt{16 - 4(4\\gamma^2 - 1)}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 = \\frac{4\\gamma - \\sqrt{16 - 4(4\\gamma^2 - 1)}}{2}\n",
    "$$\n",
    "\n",
    "For the function to be convex, both $ \\lambda_1 $ and $ \\lambda_2 $ must be non-negative, indicating that the Hessian is positive semidefinite everywhere. However, for certain values of $ \\gamma $ (especially when $ \\gamma $ is small), one of the eigenvalues can become negative. This would mean that the Hessian is not positive-definite everywhere, and as a result, the function is not convex across its entire domain.\n",
    "\n",
    "It's determined that for the range $ -\\frac{\\sqrt{10}}{4} < \\gamma < \\frac{\\sqrt{10}}{4} $, at least one eigenvalue is negative. This implies that the function is not convex for values of $ \\gamma $ within this range. This means that the function is not generally convex.\n",
    "\n",
    "## Differentiability:\n",
    "\n",
    "The loss function $ \\mathcal{L}(p, q) = -pq + \\gamma (p^2 + q^2) $ is differentiable everywhere. This is because it consists of terms that have continuous first derivatives with respect to both $ p $ and $ q $ across their entire domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-10:}}$ In the reference paper [1] above, additional bias terms are recommended, implement it\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Put your modified function here or below this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "\n",
    "class LF_b():\n",
    "    def __init__(self, R, K, alpha, gamma, iterations, bias=True):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "        self.bias = bias\n",
    "\n",
    "    def train(self, tp=0):\n",
    "        self.P = np.random.normal(\n",
    "            scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(\n",
    "            scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        self.mu = np.mean(self.R[np.where(self.R != 0)])\n",
    "        if self.bias:\n",
    "            self.b_i = np.array(\n",
    "                [np.mean(self.R[np.where(self.R[:, i] != 0)]) - self.mu if self.R[np.where(self.R[:, i] != 0)].size else 0 for i in range(self.num_items)])\n",
    "            self.b_u = np.array(\n",
    "                [np.mean(self.R[np.where(self.R[i, :] != 0)]) - self.mu if self.R[np.where(self.R[i, :] != 0)].size else 0 for i in range(self.num_users)])\n",
    "            # sum\n",
    "            self.b = np.array([np.array([self.b_u[j] + self.mu + self.b_i[i]\n",
    "                                         for i in range(self.num_items)]) for j in range(self.num_users)])\n",
    "        else:\n",
    "            self.b = 0\n",
    "            self.b_i = 0\n",
    "            self.b_u = 0\n",
    "\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            if tp == 0:\n",
    "                self.sgd()\n",
    "            else:\n",
    "                self.gd_All()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 10 == 0:\n",
    "                if tp == 0:\n",
    "                    print(\"SGD Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "                else:\n",
    "                    print(\"GD Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.predict_all_bias(i, j)\n",
    "            e = (r - prediction)\n",
    "            # print shapes\n",
    "            # ic(self.P[i, :].shape)\n",
    "            # ic(self.Q[j, :].shape)\n",
    "            # ic(e.shape)\n",
    "            # ic(self.b_u[i].shape)\n",
    "            # ic(self.b_i[j].shape)\n",
    "            # ic(self.b.shape)\n",
    "\n",
    "            self.P[i, :] += self.alpha * \\\n",
    "                (e * self.Q[j, :] - self.gamma * self.P[i, :])\n",
    "            self.Q[j, :] += self.alpha * \\\n",
    "                (e * self.P[i, :] - self.gamma * self.Q[j, :])\n",
    "\n",
    "    def gd_All(self):\n",
    "        # Initialize gradients for P and Q\n",
    "        grad_P = np.zeros((self.num_users, self.K))\n",
    "        grad_Q = np.zeros((self.num_items, self.K))\n",
    "\n",
    "        predictions = self.b_u[:, np.newaxis] + self.mu + \\\n",
    "            self.b_i[np.newaxis, :] + self.P.dot(self.Q.T)\n",
    "\n",
    "        observed_ratings_mask = (self.R > 0)\n",
    "\n",
    "        errors = (self.R - predictions) * observed_ratings_mask\n",
    "\n",
    "        # Compute gradients for P and Q using vectorized operations\n",
    "        for j in range(self.num_items):\n",
    "            grad_P += -2 * \\\n",
    "                np.outer(errors[:, j], self.Q[j, :]) + 2 * self.gamma * self.P\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            grad_Q += -2 * \\\n",
    "                np.outer(errors[i, :], self.P[i, :]) + 2 * self.gamma * self.Q\n",
    "\n",
    "        # Update user and item latent feature matrices using the computed gradients\n",
    "        self.P -= self.alpha * grad_P\n",
    "        self.Q -= self.alpha * grad_Q\n",
    "\n",
    "    def predict(self, i, j):\n",
    "        return self.mu + self.P[i, :].dot(self.Q[j, :].T)\n",
    "\n",
    "    def predict_all_bias(self, i, j):\n",
    "        return self.mu + self.b_i[j] + self.b_u[i] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "\n",
    "    def full_matrix(self):\n",
    "        return self.b + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Iteration: 10 ; error = 8.2907\n",
      "SGD Iteration: 20 ; error = 6.5189\n",
      "SGD Iteration: 30 ; error = 4.9321\n",
      "SGD Iteration: 40 ; error = 1.7808\n",
      "SGD Iteration: 50 ; error = 1.2122\n",
      "SGD Iteration: 60 ; error = 1.1452\n",
      "SGD Iteration: 70 ; error = 1.0906\n",
      "SGD Iteration: 80 ; error = 1.0363\n",
      "SGD Iteration: 90 ; error = 0.9823\n",
      "SGD Iteration: 100 ; error = 0.9291\n",
      "\n",
      "P x Q:\n",
      "[[ 5.05680944  2.61839397 -1.59796856  1.04617507]\n",
      " [ 3.94931313  2.32438539 -0.68152005  1.00472478]\n",
      " [ 0.87242405  1.55482968  3.60664241  4.89851023]\n",
      " [ 1.03987135  1.61357288  3.11242321  3.97295402]\n",
      " [-1.9449238   0.4095602   4.87620231  4.09450876]]\n",
      "\n",
      "Global bias:\n",
      "[[0.43910256 0.98076923 1.31410256 0.61410256]\n",
      " [0.60576923 1.1474359  1.48076923 0.78076923]\n",
      " [0.43910256 0.98076923 1.31410256 0.61410256]\n",
      " [0.60576923 1.1474359  1.48076923 0.78076923]\n",
      " [0.2724359  0.81410256 1.1474359  0.4474359 ]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_and_plotb(R, K=2, alpha=0.1, gamma=0.01, iterations=100, bias=True, tp=0):\n",
    "    lf = LF_b(R, K=K, alpha=alpha, gamma=gamma,\n",
    "              iterations=iterations, bias=bias)\n",
    "    training_process = lf.train(tp=tp)\n",
    "\n",
    "    print(\"\\nP x Q:\")\n",
    "    print(lf.full_matrix())\n",
    "    print(\"\\nGlobal bias:\")\n",
    "    print(lf.b)\n",
    "    print()\n",
    "\n",
    "    x = [x for x, y in training_process]\n",
    "    y = [y for x, y in training_process]\n",
    "    plt.figure(figsize=((16, 4)))\n",
    "    plt.plot(x, y)\n",
    "    plt.xticks(x, x)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Square Error\")\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_and_plotb(R, K=2, alpha=0.02, gamma=0.01, iterations=100, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Question-11:}}$ Download the netflix ratings matrix, and run your algorithm.\n",
    "\n",
    "$\\color{red}{\\text{Answer:}}$ Put your MSE versus iterations here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thats All Folks! Further Readings or Infomation\n",
    "\n",
    "https://www.coursera.org/learn/networks-illustrated/lecture/8GPZT/netflix-timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep Recommending!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
