{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem  (Assignment 3, TAO, Spring 2023)\n",
    "### Instructor: Dr. Pawan Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Problem\n",
    "### Given a set of input vectors corresponding to objects (or featues) decide which of the N classes the object belongs to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference (some figures for illustration below are taken from this): \n",
    "1. SVM without Tears, https://med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf\n",
    "2. SVM is one of the most popular methods in machine learning\n",
    "3. The dataset iris required is in the zip folder of assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Methods for Classification Problem\n",
    "1. Perceptron\n",
    "2. SVM\n",
    "3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will briefly describe the idea behind support vector machines for classification problems. We first describe linear SVM used to classify linearly separable data, and then we describe how we can use these algorithm for non-linearly separable data by so called kernels. The kernels are functions that map non-linearly separable data to a space usually higher dimensional space where the data becomes linearly separable. Let us quickly start with linear SVM.\n",
    "\n",
    "### Linear SVM for two class classification\n",
    "We recall the separating hyperpplane theorem: If there are two non-intersecting convex set, then there exists a hyperplane that separates the two convex sets. This is the assumption we will make: we assume that the convex hull of the given data leads to two convex sets for two classes, such that a hyperplane exists that separates the convex hulls. \n",
    "\n",
    "### Main idea of SVM: \n",
    "Not just find a hyperplane (as in perceptrons), but find one that keeps a good (largest possible) gap from the the data samples of each class. This gap is popularly called margins.\n",
    "\n",
    "### Illustration of problem, and kewords\n",
    "Consider the dataset of cancer and normal patients, hence it is a two class problem. Let us visualize the data:\n",
    "<img src=\"svmt1.png\" width=\"550\">\n",
    "Let us notice the following about the given data:\n",
    "0. There are two classes: blue shaded stars and red shaded circles.\n",
    "2. The input vector is two dimensional, hence it is of the form $(x_1^1, x_2^1).$\n",
    "2. Here $x_1^1, x_2^2$ are values of the features corresponding to two gene features: Gene X, Gene Y.\n",
    "3. Here red line is the linear classifier or hyperplane that separates the given input data.\n",
    "4. There are two dotted lines: one passes through a blue star point, and another dotted line passes through two red shaded circle points.\n",
    "5. The distance between the two dotted lines is called gap or margin that we mentioned before.\n",
    "6. Goal of SVM compared to perceptrons is to maximize this margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of Optimization Model for Linear SVM\n",
    "We now assume that the red hyperplane above with maximum margin is given by $$w \\cdot x + b,$$\n",
    "We further assume that the dotted lines above are given by $$w \\cdot x + b = -1, \\quad w \\cdot x + b = +1.$$\n",
    "<img src=\"svmt2.png\" width=\"400\">\n",
    "For reasons, on why we can choose such hyperplane is shown in slides Lecture 16 of TAO. Since we want to maximize the margin the distance between the dotted lines, we recall the formula for diatance between planes. Let $D$ denote the distance, then \n",
    "$$D = 2/ \\| w \\|.$$\n",
    "So, to maximize the margin $D,$ we need to minimize $\\| w \\|.$ For convenience of writing algorithm (for differentiable function), we can say that minimizing $\\| w \\|$ is equivalent to minimizing $1/2 \\| w \\|^2.$ Hence \n",
    "### Objective function: $\\dfrac{1}{2} \\| w \\|^2$\n",
    "For our hyperplane to classify correctly, we need points of one class on one side of dotted line, more concretely\n",
    "$$w \\cdot x + b \\leq -1,$$\n",
    "and the we want the samples of another class (red ones) be on the other side of other dotted lines, i.e., \n",
    "$$ w \\cdot x + b \\geq +1.$$\n",
    "Let us now look what constraints mean in figure:\n",
    "<img src=\"svmt3.png\" width=\"400\">\n",
    "With this we are all set to write the constraints for our optimization model for SVM. \n",
    "### Constraints: \n",
    "$$\n",
    "\\begin{align}\n",
    "&w \\cdot x_i + b \\leq -1, \\quad \\text{if}~y_i = -1\\\\\n",
    "&w \\cdot x_i + b \\geq +1, \\quad \\text{if}~y_i = +1\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, objective function with constraints, gives us the full model. The data for which the label $y_i$ is $-1$ satisfies $w \\cdot x + b \\leq -1,$ and the data for which the lable $y_i$ is $+1$ satisfies $w \\cdot x + b \\geq +1.$ Hence both these conditions can be combined to get\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i (w \\cdot x_i + b) \\geq 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Optimization Model (Primal Form):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} \\quad & \\dfrac{1}{2} \\| w \\|^2 \\\\\n",
    "\\text{subject to} \\quad &w \\cdot x_i + b \\geq 1, \\quad i=1,\\dots,m,\n",
    "\\end{align}\n",
    "$$\n",
    "where $m$ is the number of samples $x_i,$ and $w \\in \\mathbb{R}^n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Prove that the primal objective is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here. \n",
    "\n",
    "## Proof of Convexity for Primal SVM Objective Function\n",
    "\n",
    "Objective function:\n",
    "$$ f(w, \\xi) = \\frac{1}{2} || w ||^2 + C \\sum_{i=1}^N \\xi_i $$\n",
    "\n",
    "### 1. Convexity of $ \\frac{1}{2} || w ||^2 $\n",
    "\n",
    "For a function to be convex, its Hessian must be positive semidefinite. \n",
    "\n",
    "Given:\n",
    "$$ g(w) = \\frac{1}{2} || w ||^2 = \\frac{1}{2} w^T w $$\n",
    "\n",
    "The gradient of this function with respect to $ w $ is:\n",
    "$$ \\nabla g(w) = w $$\n",
    "\n",
    "Taking the second derivative (Hessian), we get:\n",
    "$$ H = \\nabla^2 g(w) = I $$\n",
    "\n",
    "Where $ I $ is the identity matrix. All the eigenvalues of the identity matrix are 1, which means they are all non-negative. Hence, the Hessian is positive semidefinite, implying that $ g(w) $ is convex.\n",
    "\n",
    "### 2. Convexity of $ C \\sum_{i=1}^N \\xi_i $\n",
    "\n",
    "This term is linear in $ \\xi $, and linear functions are always convex. \n",
    "\n",
    "Hence, the function:\n",
    "$$ h(\\xi) = C \\sum_{i=1}^N \\xi_i $$\n",
    "\n",
    "Has its Hessian matrix as a zero matrix, which is positive semidefinite. Thus, this term is convex.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both terms in our objective function are convex. The sum of convex functions is also convex. Therefore, the primal SVM objective function is convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Write the primal problem in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here.\n",
    "\n",
    "## Primal Form of the Support Vector Machine (SVM)\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Minimize the following function with respect to the variables $w$, $b$, and $\\xi$:\n",
    "$$ f(w, \\xi) = \\frac{1}{2} || w ||^2 + C \\sum_{i=1}^N \\xi_i $$\n",
    "\n",
    "### Constraints:\n",
    "\n",
    "1. For all data points $i$, the following must hold:\n",
    "$$ y_i (w^T x_i + b) \\geq 1 - \\xi_i $$\n",
    "\n",
    "2. Slack variables must be non-negative for all data points:\n",
    "$$ \\xi_i \\geq 0 $$\n",
    "\n",
    "Where:\n",
    "- $w$ is the weight vector.\n",
    "- $b$ is the bias term.\n",
    "- $\\xi_i$ are the slack variables introduced to allow for misclassification or violations of the margin.\n",
    "- $x_i$ are the data points.\n",
    "- $y_i$ are the corresponding labels, which can be either +1 or -1.\n",
    "- $C$ is a regularization parameter that determines the trade-off between maximizing the margin and minimizing classification errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model (Dual Form)\n",
    "The dual form was derived in lecture 16:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m{\\lambda_i} - \\dfrac{1}{2} \\sum_{i=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m{\\lambda_i y_i} = 0, \\quad i = 1, \\dots, m\n",
    "\\end{align*}, \n",
    "$$\n",
    "where $\\lambda_i$ is the Lagrange multiplier. We claim that strong duality holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Show the derivation of dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your answer here (use latex)\n",
    "\n",
    "## Derivation of the Dual Form of the SVM (Hard-margin)\n",
    "\n",
    "### Primal Problem:\n",
    "\n",
    "The primal SVM problem without slack variables can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\frac{1}{2} || w ||^2 \\\\\n",
    "&\\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Lagrangian:\n",
    "\n",
    "To derive the dual, we first form the Lagrangian. For every inequality constraint, we introduce a non-negative Lagrange multiplier $\\lambda_i$.\n",
    "\n",
    "The Lagrangian $L$ is given by:\n",
    "\n",
    "$$\n",
    "L(w, b, \\lambda) = \\frac{1}{2} || w ||^2 - \\sum_{i=1}^m \\lambda_i [y_i (w^T x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "### Conditions for Optimality:\n",
    "\n",
    "1. Gradient of $L$ w.r.t. $w$ and $b$ should be zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^m \\lambda_i y_i x_i = 0 \\implies w = \\sum_{i=1}^m \\lambda_i y_i x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^m \\lambda_i y_i = 0 \\implies \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "$$\n",
    "\n",
    "2. Complementary slackness:\n",
    "Since we are considering hard-margin SVM, all constraints are satisfied as strict inequalities. Hence, this condition is inherently satisfied.\n",
    "\n",
    "3. $\\lambda_i \\geq 0$ for all $i$.\n",
    "\n",
    "### Dual Problem:\n",
    "\n",
    "Substitute the optimal $w$ obtained from the gradient conditions into the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m{\\lambda_i} - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m{\\lambda_i y_i} = 0, \\quad i = 1, \\dots, m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is the dual form of the SVM without slack variables. It's a quadratic programming problem that can be solved using various optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Prove that strong duality holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ \n",
    "\n",
    "## Proof of Strong Duality for the SVM (Hard-margin)\n",
    "\n",
    "### Primal Problem:\n",
    "\n",
    "We start by recalling the primal SVM formulation for the hard-margin case:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\frac{1}{2} || w ||^2 \\\\\n",
    "&\\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Slater's Condition:\n",
    "\n",
    "A key tool for verifying strong duality is Slater's condition, which states:\n",
    "\n",
    "For convex optimization problems, if there exists a feasible point that lies in the interior of the primal feasible region (i.e., not on the boundary), then strong duality holds. For problems with affine constraints, the point can even be on the boundary.\n",
    "\n",
    "### Application to SVM:\n",
    "\n",
    "For the SVM problem:\n",
    "\n",
    "- The objective, $\\frac{1}{2} || w ||^2$, is quadratic and convex.\n",
    "- All constraints, specifically $y_i (w^T x_i + b) \\geq 1$, are affine.\n",
    "\n",
    "Given these conditions, Slater's criterion is trivially satisfied.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Since Slater's condition holds for the SVM with linearly separable data, we can conclude that strong duality is present. This means the optimal value of the primal SVM problem matches the optimal value of its dual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Prove that the dual objective is concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here. \n",
    "$\\color{blue}{Answer:}$ \n",
    "\n",
    "## Proof that the Dual Objective is Concave\n",
    "\n",
    "### Convex Primal Problem:\n",
    "\n",
    "Consider a general convex optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad f(x) \\\\\n",
    "&\\text{subject to} \\quad g_i(x) \\leq 0 \\quad i = 1, \\dots, m \\\\\n",
    "&\\quad \\quad \\quad \\quad h_j(x) = 0 \\quad j = 1, \\dots, p\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is a convex function.\n",
    "- $g_i(x)$ are convex functions.\n",
    "- $h_j(x)$ are affine functions.\n",
    "\n",
    "### Lagrangian:\n",
    "\n",
    "The Lagrangian associated with this problem is:\n",
    "\n",
    "$$\n",
    "L(x, \\lambda, \\nu) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^p \\nu_j h_j(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_i \\geq 0$ are the Lagrange multipliers associated with the inequality constraints.\n",
    "- $\\nu_j$ are the Lagrange multipliers associated with the equality constraints.\n",
    "\n",
    "### Dual Function:\n",
    "\n",
    "The dual function $g(\\lambda, \\nu)$ is defined as:\n",
    "\n",
    "$$\n",
    "g(\\lambda, \\nu) = \\inf_{x} L(x, \\lambda, \\nu)\n",
    "$$\n",
    "\n",
    "### Concavity of the Dual Function:\n",
    "\n",
    "For any fixed $(\\lambda, \\nu)$, the Lagrangian $L(x, \\lambda, \\nu)$ is affine in $x$ because $f(x)$ is convex, $g_i(x)$ are convex, and $h_j(x)$ are affine. \n",
    "\n",
    "The infimum of an affine function is either $-\\infty$ or the value of the function at some extreme point. This means the dual function $g(\\lambda, \\nu)$ is the pointwise infimum of a family of affine functions. A pointwise infimum of a family of affine (hence, linear) functions is always concave.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Since the dual function $g(\\lambda, \\nu)$, which defines the objective of the dual problem, is concave, we conclude that the dual objective of a convex optimization problem is always concave. And given that SVM is a convex optimization problem, the dual objective of the SVM is concave. Although this statement is true even for non-convex problems, we will restrict our discussion to convex problems only since SVM is a convex problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Write the dual problem in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here.\n",
    "\n",
    "$\\color{blue}{Answer:}$ \n",
    "\n",
    "## Dual Formulation of SVM in Standard Form\n",
    "\n",
    "### Primal Problem:\n",
    "\n",
    "For the sake of clarity, let's first recall the primal SVM problem without slack variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\frac{1}{2} || w ||^2 \\\\\n",
    "&\\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Lagrangian:\n",
    "\n",
    "For this primal problem, the associated Lagrangian is:\n",
    "\n",
    "$$\n",
    "L(w, b, \\lambda) = \\frac{1}{2} || w ||^2 - \\sum_{i=1}^m \\lambda_i [y_i (w^T x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_i \\geq 0$ are the Lagrange multipliers.\n",
    "\n",
    "### Dual Problem:\n",
    "\n",
    "By minimizing the Lagrangian $L(w, b, \\lambda)$ with respect to primal variables (w and b) and maximizing with respect to dual variables $\\lambda$, we derive the dual form. The dual problem becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0 \\\\\n",
    "&\\quad \\quad \\quad \\quad \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- The dual variables $\\lambda_i$ correspond to the constraints from the primal problem.\n",
    "- The term $\\sum_{i=1}^m \\lambda_i$ comes from the Lagrange multipliers associated with the constraints.\n",
    "- The quadratic term involving $(x_i \\cdot x_j)$ arises from the minimization of the Lagrangian with respect to w.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "This dual form provides a quadratic programming problem where the decision variables are the Lagrange multipliers $\\lambda_i$. The kernel trick in SVMs is essentially the substitution of the inner product with a kernel function in the dual problem, allowing SVMs to operate in transformed feature spaces without explicitly computing the transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin SVM\n",
    "In a variant of soft margin SVM, we assume that some data samples may be outliers or noise, and this prevents the data from being linearly separable. For example, see the figure below\n",
    "<img src=\"svmt4.png\" width=\"400\">\n",
    "In the figure, we see that \n",
    "\n",
    "- We believe that two red and one blue sample is noisy or outliers.\n",
    "- We now want to take into account that real life data is noisy, we decide to allow for some of the noisy data in the margin.\n",
    "- Let $\\xi_i$ denotes how far a data sample is from the middle plane (margin is the area between dotted line).\n",
    "- For example, one of the noisy red data point in 0.6 away from middle red plane. \n",
    "- We introduce this slack variable $\\xi_i \\geq 0$ for each data sample $x_i.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Primal Soft-Margin\n",
    "We can then write the primal soft-margin optimization model as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\dfrac{1}{2} \\| w \\|^2 + C \\sum_{i=1}^m \\xi_i \\\\\n",
    "&\\text{subject to} \\quad y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\dots, m.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Dual Soft-Margin\n",
    "We can also write the dual form of soft-margin SVM as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\dfrac{1}{2} \\sum_{i,j=1}^m \\lambda_i \\lambda_j \\: y_i y_j \\: x_i \\cdot x_j \\\\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad  i=1, \\dots, m, \\\\ \n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\t \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Show the derivation of dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your answer here (use latex)\n",
    "\n",
    "$\\color{blue}{Answer:}$ \n",
    "\n",
    "## Derivation of the Dual Formulation for Soft-Margin SVM\n",
    "\n",
    "### Primal Problem (Soft-Margin SVM):\n",
    "\n",
    "Recall the primal SVM problem with slack variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\frac{1}{2} || w ||^2 + C \\sum_{i=1}^m \\xi_i \\\\\n",
    "&\\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1 - \\xi_i \\quad \\forall i \\\\\n",
    "&\\quad \\quad \\quad \\quad \\xi_i \\geq 0 \\quad \\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Lagrangian:\n",
    "\n",
    "Given the constraints, the Lagrangian for this problem is:\n",
    "\n",
    "$$\n",
    "L(w, b, \\xi, \\lambda, r) = \\frac{1}{2} || w ||^2 + C \\sum_{i=1}^m \\xi_i - \\sum_{i=1}^m \\lambda_i [y_i (w^T x_i + b) - 1 + \\xi_i] - \\sum_{i=1}^m r_i \\xi_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_i \\geq 0$ are the Lagrange multipliers for the margin constraints.\n",
    "- $r_i \\geq 0$ are the Lagrange multipliers for the slack variable non-negativity constraints.\n",
    "\n",
    "### Derivation of Dual:\n",
    "\n",
    "To find the dual, we'll start by minimizing the Lagrangian with respect to the primal variables ($w$, $b$, and $\\xi$).\n",
    "\n",
    "1. Differentiating $L$ with respect to $w$ and setting to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^m \\lambda_i y_i x_i = 0 \\\\\n",
    "\\Rightarrow w = \\sum_{i=1}^m \\lambda_i y_i x_i\n",
    "$$\n",
    "\n",
    "2. Differentiating $L$ with respect to $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^m \\lambda_i y_i = 0 \\\\\n",
    "\\Rightarrow \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "$$\n",
    "\n",
    "3. Differentiating $L$ with respect to $\\xi_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = C - \\lambda_i - r_i = 0 \\\\\n",
    "\\Rightarrow \\lambda_i + r_i = C\n",
    "$$\n",
    "\n",
    "Inserting these results into the Lagrangian, and considering that $r_i \\geq 0$ which implies $\\lambda_i \\leq C$, we get the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad 0 \\leq \\lambda_i \\leq C \\\\\n",
    "&\\quad \\quad \\quad \\quad \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "In the Soft-Margin SVM, the inclusion of slack variables introduces an upper bound of C on the Lagrange multipliers in the dual. This upper bound, together with the lower bound of zero, results in a box constraint for the multipliers in the dual problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ List advantages of dual over primal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your answer here (use latex)\n",
    "\n",
    "$\\color{blue}{Answer:}$ \n",
    "\n",
    "## Advantages of the Dual Formulation Over the Primal for SVM\n",
    "\n",
    "1. **Kernel Trick Applicability**:\n",
    "    \n",
    "    One of the most notable advantages of the dual formulation is its compatibility with the Kernel trick. In the dual problem, data points appear as dot products $(x_i \\cdot x_j)$. This allows for the introduction of kernel functions $K(x_i, x_j)$, enabling SVM to efficiently handle non-linearly separable data by implicitly mapping data to a higher-dimensional space.\n",
    "\n",
    "    $$ K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j) $$\n",
    "\n",
    "    Here, $\\phi$ is the implicit mapping function. Using kernel functions, we can compute these dot products in the higher-dimensional space without explicitly performing the transformation, thus avoiding potentially high computational costs.\n",
    "\n",
    "2. **Sparsity of Solution**:\n",
    "\n",
    "    In the dual SVM, most of the Lagrange multipliers ($\\lambda_i$) will be zero. Data points associated with non-zero multipliers are termed as \"support vectors\". This means that the solution is determined only by a subset of the data, the support vectors, making the solution sparse and reducing the storage and computational requirements.\n",
    "\n",
    "3. **Efficiency in Large Dimensional Data**:\n",
    "\n",
    "    In cases where the number of features (dimensions) is much larger than the number of samples, the dual problem can be more efficient. This is because the complexity of solving the dual is often proportional to the number of samples squared or cubed, while the primal's complexity might be influenced more directly by the number of features.\n",
    "\n",
    "4. **Parallel Computations**:\n",
    "\n",
    "    Dual methods, especially decomposition methods used to solve SVM, can be implemented in a parallel manner. This allows for scalability and efficient processing of large datasets on modern hardware architectures.\n",
    "\n",
    "5. **Explicit Regularization**:\n",
    "\n",
    "    In the soft-margin SVM dual, the box constraint on the Lagrange multipliers ($0 \\leq \\lambda_i \\leq C$) provides explicit regularization. This helps control the model's capacity, and thus its generalization ability, by setting an appropriate value of C.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "While both primal and dual formulations have their own advantages, the ability of the dual to harness the kernel trick, its sparse solutions, and its efficiency in certain scenarios make it particularly attractive for SVMs, especially when handling non-linear data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels in SVM\n",
    "## Non-Linear Classifiers\n",
    "- For nonlinear data, we may map the data to a higher dimensional feature space where it is separable. See the figure below:\n",
    "<img src=\"svmt5.png\" width=\"700\">\n",
    "Such non-linear transformation can be implemented more effectively using the dual formulation. \n",
    "- If we solve the dual form of linear SVM, then the predictions is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\text{sign}(w \\cdot x + b) \\\\\n",
    "w &= \\sum_{i=1}^m \\alpha_i y_i x_i \n",
    "\\end{align*}\n",
    "$$\n",
    "If we assume that we did some transform $\\Phi,$ then the classifier is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\text{sign} (w \\cdot \\Phi(x) + b) \\\\\n",
    "w &= \\sum_{i=1}^m \\alpha_i y_i \\Phi(x_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "If we substitute $w$ in $f(x),$ we observe that\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) = \\text{sign} \\left ( \\sum_{i=1}^m \\alpha_i y_i \\, \\Phi(x_i) \\cdot \\Phi(x) + b \\right) = \\text{sign} \\left( \\sum_{i=1}^m \\alpha_i y_i \\, K(x_i, x) + b \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "Note that doing dot products such as $\\Phi(x_i) \\cdot \\Phi(x),$ if $\\Phi(x)$ is a long vector! An important observation is to define this dot product or $K(x,z)$ such that dot products happen in input space rather than the feature space. We can see this with following example:\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(x \\cdot z) &= (x \\cdot z)^2 = \\left( \\begin{bmatrix}\n",
    "x_{(1)} \\\\ x_{(2)} \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "z_{(1)} \\\\ z_{(2)}\n",
    "\\end{bmatrix} \\right)^2 = (x_{(1)} z_{(1)} + x_{(2)} z_{(2)})^2 \\\\\n",
    "&= x_{(1)}^2 z_{(1)}^2 + 2x_{(1)} z_{(1)} x_{(2)} z_{(2)} + x_{(2)}^2 z_{(2)}^2 = \\begin{bmatrix}\n",
    "x_{(1)}^2 \\\\ \\sqrt{2} x_{(1)} x_{(2)} \\\\ x_{(2)}^2 \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "z_{(1)}^2 \\\\ \\sqrt{2} z_{(1)} z_{(2)} \\\\ z_{(2)}^2 \n",
    "\\end{bmatrix}  \\\\\n",
    "&= \\Phi(x) \\cdot \\Phi(z)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Let the kernel be defined by $K(x,z) = (x \\cdot z)^3.$ Define $\\Phi(x).$ Assuming that one multiplications is 1 FLOP, and one addition is 1 FLOP, then how many flops you need to compute $K(x \\cdot z)$ in input space versus feature space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Write your answer in this cell.\n",
    "\n",
    "Given the kernel:\n",
    "$$ K(x,z) = (x \\cdot z)^3 $$\n",
    "\n",
    "For a two-dimensional vector $ x = \\begin{bmatrix} x_{(1)} \\\\ x_{(2)} \\end{bmatrix} $ and $ z = \\begin{bmatrix} z_{(1)} \\\\ z_{(2)} \\end{bmatrix} $, the kernel is expanded as:\n",
    "\n",
    "$$ K(x,z) = (x_{(1)} z_{(1)} + x_{(2)} z_{(2)})^3 $$\n",
    "\n",
    "This expands to:\n",
    "\n",
    "$$ K(x,z) = x_1^3 z_1^3 + 3 x_1^2 x_2 z_1^2 z_2 + 3 x_1 x_2^2 z_1 z_2^2 + x_2^3 z_2^3 $$\n",
    "\n",
    "We can express this in terms of a dot product $ \\Phi(x) \\cdot \\Phi(z) $, where:\n",
    "\n",
    "$$ \\Phi(x) = \\begin{bmatrix} x_1^3 \\\\ \\sqrt{3} x_1^2 x_2 \\\\ \\sqrt{3} x_1 x_2^2 \\\\ x_2^3 \\end{bmatrix} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\Phi(z) = \\begin{bmatrix} z_1^3 \\\\ \\sqrt{3} z_1^2 z_2 \\\\ \\sqrt{3} z_1 z_2^2 \\\\ z_2^3 \\end{bmatrix} $$\n",
    "\n",
    "### FLOP Count:\n",
    "\n",
    "1. **Input Space**: Note, this computation needs to be performed in order and we store all intermediate results.\n",
    "   - Computing $ x_1^3 z_1^3 $ requires 3 multiplications.\n",
    "   - Computing $ x_2^3 z_2^3 $ requires 3 multiplications.\n",
    "   - Computing $ 3 x_1^2 x_2 z_1^2 z_2 $ requires 3 multiplications.\n",
    "   - Computing $ 3 x_1 x_2^2 z_1 z_2^2 $ requires 3 multiplications.\n",
    "   - There are 3 additions between the terms.\n",
    "   Total FLOPs for Input Space = 12 multiplications + 3 additions = 15 FLOPs.\n",
    "\n",
    "2. **Feature Space**:\n",
    "   - The dot product $ \\Phi(x) \\cdot \\Phi(z) $ requires 4 multiplications (one for each term).\n",
    "   - There are 3 additions between the terms.\n",
    "   Total FLOPs for Feature Space = 4 multiplications + 3 additions = 7 FLOPs.\n",
    "\n",
    "In conclusion:\n",
    "- To compute $ K(x, z) $ in the input space, 21 FLOPs are required.\n",
    "- To compute $ K(x, z) $ in the feature space, 7 FLOPs are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Dual Soft Margin Kernel SVM\n",
    "We can now write the dual form of soft-margin Kernel SVM as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\dfrac{1}{2} \\sum_{i, \\, j=1}^m \\lambda_i \\lambda_j \\: y_i y_j \\: \\Phi(x_i) \\cdot \\Phi(x_j) \\\\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad  i=1, \\dots, m, \\\\ \n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\t \n",
    "\\end{align*}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver for Optimization Problem: Quadratic Programming\n",
    "We aspire to solve the above optimization problem using existing quadraric programming library. But we have a problem: the standard libraries use the standard form of the quadratic optimization problem that looks like the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad &\\dfrac{1}{2} x^T P x + q^T x, \\\\ \n",
    "\\text{subject to} \\quad &Gx \\leq h, \\\\\n",
    "&Ax = b\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Soft-Margin Kernel SVM in Standard QP: Assemble Matrices Vectors\n",
    "To put the dual Kernel SVM in standard form, we need to set\n",
    "- matrix $P$\n",
    "- vector $x$\n",
    "- vector $q$\n",
    "- vector $h$\n",
    "- vector $b$\n",
    "- matrix $G$\n",
    "- matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $P$\n",
    "Let $$K(x_i, x_j) = \\Phi(x_i) \\cdot \\Phi(x_j),$$ and set $(i,j)$ entry of matrix $P$ as $$P_{ij} = y_iy_j K(x_i,x_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $x$\n",
    "Set $$x = \\begin{bmatrix}\n",
    "\\lambda_1 \\\\\n",
    "\\lambda_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $q$\n",
    "Set $q \\in \\mathbb{R}^m$\n",
    "$$ q = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\ -1 \\\\ \\vdots \\\\ -1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix $A$\n",
    "Set the matrix (in fact vector) $A$ as \n",
    "$$\n",
    "A = [y_1, y_2, \\dots, y_m]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $b$\n",
    "In fact vector $b$ is a scalar here: $$b = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix $G$\n",
    "$$\n",
    "\\begin{align*}\n",
    "G = \\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\dots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 1 \\\\ \\hline\n",
    "-1 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\dots & \\vdots \\\\\n",
    "0 & 0 & \\dots& -1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $h$\n",
    "Set $h$ as \n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "C \\\\\n",
    "C \\\\\n",
    "\\vdots \\\\\n",
    "C \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Kernel SVM\n",
    "We are all set to try out coding the classifier using Kernel SVM. We will first import some libraries. Some of these libraries may not be available in your system. You may install them as follows:\n",
    "- conda install numpy\n",
    "- conda install -c conda-forge cvxopt\n",
    "- sudo apt-get install python-scipy python-matplotlib\n",
    "\n",
    "Try google search, if these does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import cvxopt as cvxopt\n",
    "from cvxopt import solvers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a class: svm \n",
    "This class will have the following functions:\n",
    "- __init__: where we will define initial default parameters\n",
    "- *construct_kernel*: here we will define some kernels such as polynomial and RBF (radial basis or Gaussian kernel)\n",
    "- *train_kernel_svm*: Here we will train, i.e, we will call a quadratic programming solver from cvxopt\n",
    "- *classify*: Here we will test our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Fill the TODO below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm:\n",
    "\n",
    "    def __init__(self, kernel='linear', C=3, sigma=1., degree=1., threshold=1e-5):\n",
    "        self.kernel = kernel\n",
    "        if self.kernel == 'linear':\n",
    "            self.degree = 1.\n",
    "            self.kernel = 'poly'\n",
    "\n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.threshold = threshold\n",
    "        self.degree = degree\n",
    "\n",
    "    def construct_kernel(self, X):\n",
    "        self.K = np.dot(X, X.T)\n",
    "\n",
    "        if self.kernel == 'poly':\n",
    "            self.K = (1. + 1./self.sigma*self.K)**self.degree\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            self.xsquared = (np.diag(self.K)*np.ones((1, self.N))).T\n",
    "            b = np.ones((self.N, 1))\n",
    "            self.K -= 0.5*(np.dot(self.xsquared, b.T) +\n",
    "                           np.dot(b, self.xsquared.T))\n",
    "            self.K = np.exp(self.K/(2.*self.sigma**2))\n",
    "\n",
    "    def train_kernel_svm(self, X, targets):\n",
    "        self.N = np.shape(X)[0]\n",
    "        self.construct_kernel(X)\n",
    "\n",
    "        # Assemble the matrices for the constraints\n",
    "        P = cvxopt.matrix(np.outer(targets, targets) * self.K)\n",
    "        q = cvxopt.matrix(-1. * np.ones(self.N))\n",
    "        A = cvxopt.matrix(targets, (1, self.N), tc='d')\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        # If no regularization parameter is provided, it's a hard-margin SVM\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(-1. * np.ones(self.N)))\n",
    "            h = cvxopt.matrix(np.zeros(self.N))\n",
    "        # Soft-margin SVM\n",
    "        else:\n",
    "            G = cvxopt.matrix(\n",
    "                np.vstack((-1. * np.eye(self.N), np.eye(self.N))))\n",
    "            h = cvxopt.matrix(\n",
    "                np.hstack((np.zeros(self.N), self.C * np.ones(self.N))))\n",
    "\n",
    "        # Call the the quadratic solver of cvxopt library.\n",
    "        sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Get the Lagrange multipliers out of the solution dictionary\n",
    "        lambdas = np.array(sol['x'])\n",
    "\n",
    "        # Find the (indices of the) support vectors, which are the vectors with non-zero Lagrange multipliers\n",
    "        self.sv = np.where(lambdas > self.threshold)[0]\n",
    "        self.nsupport = len(self.sv)\n",
    "        print(\"Number of support vectors = \", self.nsupport)\n",
    "\n",
    "        # Keep the data corresponding to the support vectors\n",
    "        self.X = X[self.sv, :]\n",
    "        self.lambdas = lambdas[self.sv]\n",
    "        self.targets = targets[self.sv]\n",
    "\n",
    "        self.b = np.sum(self.targets)\n",
    "        for n in range(self.nsupport):\n",
    "            self.b -= np.sum(self.lambdas*self.targets *\n",
    "                             np.reshape(self.K[self.sv[n], self.sv], (self.nsupport, 1)))\n",
    "        self.b /= len(self.lambdas)\n",
    "\n",
    "        if self.kernel == 'poly':\n",
    "            def classify(Y, soft=False):\n",
    "                K = (1. + 1./self.sigma*np.dot(Y, self.X.T))**self.degree\n",
    "\n",
    "                self.y = np.zeros((np.shape(Y)[0], 1))\n",
    "                for j in range(np.shape(Y)[0]):\n",
    "                    for i in range(self.nsupport):\n",
    "                        self.y[j] += self.lambdas[i]*self.targets[i]*K[j, i]\n",
    "                    self.y[j] += self.b\n",
    "\n",
    "                if soft:\n",
    "                    return self.y\n",
    "                else:\n",
    "                    return np.sign(self.y)\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            def classify(Y, soft=False):\n",
    "                K = np.dot(Y, self.X.T)\n",
    "                c = (1./self.sigma * np.sum(Y**2, axis=1)\n",
    "                     * np.ones((1, np.shape(Y)[0]))).T\n",
    "                c = np.dot(c, np.ones((1, np.shape(K)[1])))\n",
    "                aa = np.dot(self.xsquared[self.sv],\n",
    "                            np.ones((1, np.shape(K)[0]))).T\n",
    "                K = K - 0.5*c - 0.5*aa\n",
    "                K = np.exp(K/(2.*self.sigma**2))\n",
    "\n",
    "                self.y = np.zeros((np.shape(Y)[0], 1))\n",
    "                for j in range(np.shape(Y)[0]):\n",
    "                    for i in range(self.nsupport):\n",
    "                        self.y[j] += self.lambdas[i]*self.targets[i]*K[j, i]\n",
    "                    self.y[j] += self.b\n",
    "\n",
    "                if soft:\n",
    "                    return self.y\n",
    "                else:\n",
    "                    return np.sign(self.y)\n",
    "        else:\n",
    "            print(\"Error: Invalid kernel\")\n",
    "            return\n",
    "\n",
    "        self.classify = classify\n",
    "\n",
    "\n",
    "# Turning off the output of cvxopt solver for clarity\n",
    "cvxopt.solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ How $b$ was computed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Write your answer here.\n",
    "\n",
    "\n",
    "In the SVM implementation, the bias term $b$ is computed as follows:\n",
    "\n",
    "### 1. Initialization:\n",
    "Initialize $b$ using the sum of the target values of the support vectors:\n",
    "$$ b = \\sum_{n \\in \\text{support vectors}} \\text{target}[n] $$\n",
    "\n",
    "This step gives an initial estimate of the bias based on the target values of the support vectors.\n",
    "\n",
    "### 2. Adjusting for Lagrange multipliers and kernel values:\n",
    "For each support vector $n$, subtract from $b$ the sum of the product of:\n",
    "- The Lagrange multipliers ($\\lambda$)\n",
    "- The target values\n",
    "- The kernel values of that support vector with all other support vectors:\n",
    "\n",
    "$$ b -= \\sum_{m \\in \\text{support vectors}} \\lambda[m] \\times \\text{target}[m] \\times K[n,m] $$\n",
    "\n",
    "This adjustment ensures that the decision function correctly classifies the support vectors. The kernel values, combined with the Lagrange multipliers and target values, capture the influence of each support vector on the decision boundary.\n",
    "\n",
    "### 3. Averaging:\n",
    "Finally, divide $b$ by the total number of support vectors to get the average value:\n",
    "$$ b = \\frac{b}{\\text{number of support vectors}} $$\n",
    "\n",
    "This step ensures that the bias term is normalized with respect to the number of support vectors, providing a more generalized bias term for the SVM.\n",
    "\n",
    "The corresponding code for these steps is:\n",
    "```python\n",
    "self.b = np.sum(self.targets)\n",
    "for n in range(self.nsupport):\n",
    "    self.b -= np.sum(self.lambdas * self.targets * np.reshape(self.K[self.sv[n], self.sv], (self.nsupport, 1)))\n",
    "self.b /= len(self.lambdas)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Classifier\n",
    "In the following, we will now test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "iris = np.loadtxt('iris_proc.data', delimiter=',')\n",
    "imax = np.concatenate((iris.max(axis=0)*np.ones((1, 5)),\n",
    "                       iris.min(axis=0)*np.ones((1, 5))), axis=0).max(axis=0)\n",
    "\n",
    "target = -np.ones((np.shape(iris)[0], 3), dtype=float)\n",
    "indices = np.where(iris[:, 4] == 0)\n",
    "target[indices, 0] = 1.\n",
    "indices = np.where(iris[:, 4] == 1)\n",
    "target[indices, 1] = 1.\n",
    "indices = np.where(iris[:, 4] == 2)\n",
    "target[indices, 2] = 1.\n",
    "\n",
    "train = iris[::2, 0:4]\n",
    "traint = target[::2]\n",
    "test = iris[1::2, 0:4]\n",
    "testt = target[1::2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of support vectors =  9\n",
      "Number of support vectors =  19\n",
      "Number of support vectors =  20\n"
     ]
    }
   ],
   "source": [
    "# Training the machines\n",
    "output = np.zeros((np.shape(test)[0], 3))\n",
    "\n",
    "# Train for the first set of train data\n",
    "#svm0 = svm(kernel='linear')\n",
    "#svm0 = svm(kernel='linear')\n",
    "#svm0 = svm.svm(kernel='poly',C=0.1,degree=1)\n",
    "svm0 = svm(kernel='rbf')\n",
    "svm0.train_kernel_svm(train, np.reshape(traint[:, 0], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 0] = svm0.classify(test, soft=True).T\n",
    "\n",
    "# Train for the second set of train data\n",
    "#svm1 = svm(kernel='linear')\n",
    "#svm1 = svm(kernel='linear')\n",
    "#svm1 = svm(kernel='poly',degree=3)\n",
    "svm1 = svm(kernel='rbf')\n",
    "svm1.train_kernel_svm(train, np.reshape(traint[:, 1], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 1] = svm1.classify(test, soft=True).T\n",
    "\n",
    "# Train for the third set of train data\n",
    "#svm2 = svm(kernel='linear')\n",
    "#svm2 = svm(kernel='linear')\n",
    "#svm2 = svm(kernel='poly',C=0.1,degree=1)\n",
    "svm2 = svm(kernel='rbf')\n",
    "svm2.train_kernel_svm(train, np.reshape(traint[:, 2], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 2] = svm2.classify(test, soft=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2.]\n",
      "Misclassified locations:\n",
      "[41]\n",
      "0.9866666666666667 test accuracy\n"
     ]
    }
   ],
   "source": [
    "# Make a decision about which class\n",
    "# Pick the one with the largest margin\n",
    "bestclass = np.argmax(output, axis=1)\n",
    "print (bestclass)\n",
    "print (iris[1::2, 4])\n",
    "print(\"Misclassified locations:\")\n",
    "err = np.where(bestclass != iris[1::2, 4])[0]\n",
    "print (err)\n",
    "print (float(np.shape(testt)[0] - len(err)) /\n",
    "       (np.shape(testt)[0]), \"test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ The IRIS dataset has three classes. Explain by observing the code above how the two class SVM was modified for multiclass classification.\n",
    "\n",
    "The given code demonstrates the use of Support Vector Machines (SVMs) for multi-class classification on the IRIS dataset. The dataset has three classes. However, the standard SVM is a binary classifier, designed to differentiate between two classes. To handle the three-class scenario of the IRIS dataset, the code employs a technique called \"One-vs-Rest\" (OvR) or \"One-vs-All\" (OvA) classification.\n",
    "\n",
    "### One-vs-Rest Classification:\n",
    "\n",
    "In the One-vs-Rest approach, for a dataset with $N$ classes, we train $N$ separate SVMs. For each SVM:\n",
    "1. One class is treated as the positive class.\n",
    "2. All other classes are combined and treated as the negative class.\n",
    "\n",
    "The given code follows this strategy for the three classes in the IRIS dataset:\n",
    "\n",
    "1. **First SVM (`svm0`):** Classifies the first class versus the other two.\n",
    "```python\n",
    "svm0 = svm(kernel='rbf')\n",
    "svm0.train_kernel_svm(train, np.reshape(traint[:, 0], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 0] = svm0.classify(test, soft=True).T\n",
    "```\n",
    "\n",
    "2. **Second SVM (`svm1`):** Classifies the second class versus the other two.\n",
    "```python\n",
    "svm1 = svm(kernel='rbf')\n",
    "svm1.train_kernel_svm(train, np.reshape(traint[:, 1], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 1] = svm1.classify(test, soft=True).T\n",
    "```\n",
    "\n",
    "3. **Third SVM (`svm2`):** Classifies the third class versus the other two.\n",
    "```python\n",
    "svm2 = svm(kernel='rbf')\n",
    "svm2.train_kernel_svm(train, np.reshape(traint[:, 2], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 2] = svm2.classify(test, soft=True).T\n",
    "```\n",
    "\n",
    "### Decision Making:\n",
    "\n",
    "Once all SVMs are trained, for a new instance, each SVM gives a score (or a distance from the decision boundary). The class corresponding to the SVM that gives the highest score is chosen as the final class for the instance.\n",
    "\n",
    "In the code, this decision-making process is done by:\n",
    "```python\n",
    "bestclass = np.argmax(output, axis=1)\n",
    "```\n",
    "\n",
    "`bestclass` contains the predicted class labels based on which SVM gave the highest score for each test instance.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "By employing the One-vs-Rest strategy, the binary SVM classifier is effectively adapted for multi-class classification. Each class gets its turn to be the \"positive\" class while all other classes are treated as the \"negative\" class. The final class decision for a new instance is based on which SVM gives the highest score for that instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Write mathematical expressions for the kernels defined above.\n",
    "\n",
    "The SVM implementation provided uses two types of kernel functions:\n",
    "\n",
    "### 1. Polynomial Kernel:\n",
    "The polynomial kernel is defined as:\n",
    "$$ K(x, y) = (1 + \\frac{1}{\\sigma} x \\cdot y)^{\\text{degree}} $$\n",
    "\n",
    "Where:\n",
    "- $x$ and $y$ are the input vectors.\n",
    "- $\\sigma$ is a scaling factor.\n",
    "- $\\text{degree}$ is the degree of the polynomial.\n",
    "\n",
    "### 2. Radial Basis Function (RBF) or Gaussian Kernel:\n",
    "The RBF kernel is defined as:\n",
    "$$ K(x, y) = \\exp \\left( -\\frac{\\| x - y \\|^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "Where:\n",
    "- $x$ and $y$ are the input vectors.\n",
    "- $\\sigma$ is the width of the Gaussian function.\n",
    "\n",
    "The RBF kernel is particularly useful for data that is not linearly separable. It maps the input data into a higher-dimensional space where it might be linearly separable, allowing the SVM to find a hyperplane that separates the classes.\n",
    "\n",
    "Both of these kernels allow the SVM to learn non-linear decision boundaries, making it more flexible and capable of handling a wider range of data distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Play with different Kernels. Which kernels (polynomial, RBF, or polynomial) give the best test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel gives us the best results amongst the three kernels. This might be due to the flexibility of the RBF kernel which is known to capture complex relationships in the data. Because the RBF kernels allows for non-linear decision boundaries (as explained above), the kernel results in a more flexible classifier that can handle a wider range of data distributions. This flexibility is reflected in the higher test accuracy of the RBF kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
